{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ClearML integration"
      ],
      "metadata": {
        "id": "pNQf5neQnYPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLOps: integrate clearml in your experiment\n",
        "### it's very important to track all our experiemnt, results , data, code & model versioning for presentation."
      ],
      "metadata": {
        "id": "xsOohZ6JCTFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## docs - How to use in colab\n",
        "### \"https://clear.ml/docs/latest/docs/guides/ide/google_colab/#:~:text=Go%20to%20your%20Settings%20page,the%20information%20that%20pops%20up.&text=For%20additional%20options%20for%20running,Orchestration%20page%20of%20your%20server.\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Uzk42ZlpCuwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### credential for clearml your experiment\n",
        "     key='DIL5YU0YE1NNR1IDFPHIRGQVDTSUP8',\n",
        "     secret='9lUrgFOsh6pz57mY54DgjekKCwF-7jCbBRSc8nAWnaNGPWkODPLK62qBQ89igm7O-yo'"
      ],
      "metadata": {
        "id": "TwE02LUWDKk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXt8aGD2Dc9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define your project name and task name or can use below.\n",
        "#### create a sepate project name if you are doing independent experiment"
      ],
      "metadata": {
        "id": "FEDf_EDfDzk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### define your project name and task name or can use below.\n",
        "#### create a sepate project name if you are doing independent experiment\n",
        "\n",
        "from clearml import Task\n",
        "\n",
        "# Initialize ClearML task\n",
        "task = Task.init(\n",
        "    project_name=\"GenAI_OpenStack_RCA\",  # Project name\n",
        "    task_name=\"train_AnomalyModel\",     # Task name\n",
        "    task_type=Task.TaskTypes.training   # Task type\n",
        ")"
      ],
      "metadata": {
        "id": "DGg53bkfDcqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hsUAMkq2DNg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to run in colab"
      ],
      "metadata": {
        "id": "6efaiGLpD1jO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVseETObD1U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/allegroai/clearml\n",
        "!pip install clearml-agent"
      ],
      "metadata": {
        "id": "OVw-gWS7aLqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! export MPLBACKEND=TkAg"
      ],
      "metadata": {
        "id": "DARt-37mdaKX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from clearml import Task\n",
        "\n",
        "Task.set_credentials(\n",
        "     api_host=\"https://api.clear.ml\",\n",
        "     web_host=\"https://app.clear.ml\",\n",
        "     files_host=\"https://files.clear.ml\",\n",
        "     key='DIL5YU0YE1NNR1IDFPHIRGQVDTSUP8',\n",
        "     secret='9lUrgFOsh6pz57mY54DgjekKCwF-7jCbBRSc8nAWnaNGPWkODPLK62qBQ89igm7O-yo'\n",
        ")"
      ],
      "metadata": {
        "id": "L8KlzUfFdbF0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!clearml-agent daemon --queue default &"
      ],
      "metadata": {
        "id": "k6O6cJIyeYQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create example dataset\n",
        "from clearml import StorageManager, Dataset\n",
        "\n",
        "# # Download sample csv file\n",
        "# csv_file = StorageManager.get_local_copy(\n",
        "#     remote_url=\"https://vincentarelbundock.github.io/Rdatasets/csv/AER/Affairs.csv\"\n",
        "# )\n",
        "\n",
        "# Create a dataset with ClearML`s Dataset class\n",
        "dataset = Dataset.create(\n",
        "    dataset_project=\"GenAI_OpenStack_RCA\", dataset_name=\"OpenStack_2k.log_structured\"\n",
        ")\n",
        "\n",
        "# add the example csv\n",
        "dataset.add_files(path='/content/OpenStack_2k.log_structured.csv')\n",
        "\n",
        "# Upload dataset to ClearML server (customizable)\n",
        "dataset.upload()\n",
        "\n",
        "# commit dataset changes\n",
        "dataset.finalize()"
      ],
      "metadata": {
        "id": "UksV8vLSk3Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from clearml import Task\n",
        "\n",
        "# Initialize ClearML task\n",
        "task = Task.init(\n",
        "    project_name=\"GenAI_OpenStack_RCA\",  # Project name\n",
        "    task_name=\"train_AnomalyModel\",     # Task name\n",
        "    task_type=Task.TaskTypes.training   # Task type\n",
        ")"
      ],
      "metadata": {
        "id": "keO9N9t8pWPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train a model with clearmltracking\n",
        "from clearml import Logger\n",
        "import clearml\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "    def train_model(self, log_file, epochs=3, batch_size=16, learning_rate=2e-5):\n",
        "        # Prepare data\n",
        "        dataset = self.prepare_data(log_file, batch_size)\n",
        "\n",
        "        # Compile the model\n",
        "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Create a custom ClearML callback\n",
        "        class ClearMLCallback(Callback):\n",
        "            def on_epoch_end(self, epoch, logs=None):\n",
        "                # Log training metrics\n",
        "                Logger.current_logger().report_scalar(\"loss\", \"train\", iteration=epoch, value=logs[\"loss\"])\n",
        "                Logger.current_logger().report_scalar(\"accuracy\", \"train\", iteration=epoch, value=logs[\"accuracy\"])\n",
        "\n",
        "        # Add ClearML callback\n",
        "        clearml_callback = ClearMLCallback()\n",
        "\n",
        "        # Train the model\n",
        "        self.model.fit(dataset, epochs=epochs, callbacks=[clearml_callback])\n",
        "\n",
        "        # Save and log the model artifact\n",
        "        model_path = \"trained_model.h5\"\n",
        "        self.model.save(model_path)  # Save the model locally\n",
        "        task.upload_artifact(\"Trained_Model\", artifact_object=model_path)  # Log model to ClearML"
      ],
      "metadata": {
        "id": "yPvf5aRB_o5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add simple test logs/metrics etc in clearml\n",
        "from clearml import Task\n",
        "from clearml import Logger\n",
        "import clearml\n",
        "\n",
        "def load_model():\n",
        "    # Define the model architecture\n",
        "    model = LogAnomalyDetector(base_model='bert-base-uncased')\n",
        "\n",
        "    # Log the model structure (architecture) as text\n",
        "    task.get_logger().report_text(\"Model Architecture:\\n\" + str(model))\n",
        "\n",
        "    # Load the state dictionary into the model\n",
        "    state_dict_path = './logbert_anomaly_model_tf/model.pth'\n",
        "    model.load_state_dict(\n",
        "      torch.load(state_dict_path, map_location=torch.device('cpu'), weights_only=True)\n",
        "    )\n",
        "    model.eval()\n",
        "\n",
        "    # Log the model file as an artifact\n",
        "    task.upload_artifact(\"LogAnomalyDetector Model State\", artifact_object=state_dict_path)\n",
        "\n",
        "    print(\"Model loaded and logged in ClearML:\", model)\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i8uF0mI-_77H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train binary clasification model for anomaly - unsupervised model"
      ],
      "metadata": {
        "id": "RXbRHnuhB0eP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lqjVlpMLB0Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from clearml import Logger\n",
        "import clearml\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class LogAnomalyDetector(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Anomaly Detection Model based on BERT with TensorFlow/Keras\n",
        "    \"\"\"\n",
        "    def __init__(self, base_model_name='bert-base-uncased', max_length=128, **kwargs): # Added **kwargs to handle extra arguments\n",
        "        super(LogAnomalyDetector, self).__init__(**kwargs) # Pass extra arguments to the superclass\n",
        "        self.max_length = max_length\n",
        "        self.bert = TFBertModel.from_pretrained(base_model_name)\n",
        "        self.anomaly_head = models.Sequential([\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Access input_ids and attention_mask using keys instead of unpacking\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "        bert_outputs = self.bert(input_ids, attention_mask=attention_mask, training=training)\n",
        "        cls_embedding = bert_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "        return self.anomaly_head(cls_embedding)\n",
        "\n",
        "\n",
        "class LogAnomalyDetectionPipeline:\n",
        "    \"\"\"\n",
        "    End-to-end pipeline for log anomaly detection using TensorFlow\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name='bert-base-uncased', max_length=128, save_path='./logbert_anomaly_model_tf'):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = LogAnomalyDetector(base_model_name=model_name, max_length=max_length)\n",
        "        self.save_path = save_path\n",
        "        self.max_length = max_length\n",
        "\n",
        "    # def prepare_data(self, log_file, batch_size=16):\n",
        "    #     # Load log data\n",
        "    #     log_data = pd.read_csv(log_file)\n",
        "    #     logs = log_data['EventTemplate'].tolist()\n",
        "\n",
        "    #     # Tokenize logs\n",
        "    #     inputs = self.tokenizer(\n",
        "    #         logs,\n",
        "    #         max_length=self.max_length,\n",
        "    #         padding='max_length',\n",
        "    #         truncation=True,\n",
        "    #         return_tensors='tf'\n",
        "    #     )\n",
        "\n",
        "    #     dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    #         {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask']},\n",
        "    #         tf.zeros(len(logs))  # Dummy labels for unsupervised learning\n",
        "    #     )).batch(batch_size)\n",
        "    #     return\n",
        "\n",
        "    def prepare_data(self, log_file, batch_size=16):\n",
        "        # Load log data\n",
        "        log_data = pd.read_csv(log_file)\n",
        "        logs = log_data['EventTemplate'].tolist()\n",
        "\n",
        "        # Tokenize logs\n",
        "        inputs = self.tokenizer(\n",
        "            logs,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((\n",
        "            {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask']},\n",
        "            tf.zeros(len(logs))  # Dummy labels for unsupervised learning\n",
        "        )).batch(batch_size)\n",
        "        return dataset # Added return statement to return the dataset\n",
        "\n",
        "\n",
        "    def train_model(self, log_file, epochs=3, batch_size=16, learning_rate=2e-5):\n",
        "        # Prepare data\n",
        "        dataset = self.prepare_data(log_file, batch_size)\n",
        "\n",
        "        # Compile the model\n",
        "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Create a custom ClearML callback\n",
        "        class ClearMLCallback(Callback):\n",
        "            def on_epoch_end(self, epoch, logs=None):\n",
        "                # Log training metrics\n",
        "                Logger.current_logger().report_scalar(\"loss\", \"train\", iteration=epoch, value=logs[\"loss\"])\n",
        "                Logger.current_logger().report_scalar(\"accuracy\", \"train\", iteration=epoch, value=logs[\"accuracy\"])\n",
        "\n",
        "        # Add ClearML callback\n",
        "        clearml_callback = ClearMLCallback()\n",
        "\n",
        "        # Train the model\n",
        "        self.model.fit(dataset, epochs=epochs, callbacks=[clearml_callback])\n",
        "\n",
        "        # Save and log the model artifact\n",
        "        model_path = \"trained_model.h5\"\n",
        "        self.model.save(model_path)  # Save the model locally\n",
        "        task.upload_artifact(\"Trained_Model\", artifact_object=model_path)  # Log model to ClearML\n",
        "\n",
        "\n",
        "    # def train_model(self, log_file, epochs=3, batch_size=16, learning_rate=2e-5):\n",
        "    #     # Prepare data\n",
        "    #     dataset = self.prepare_data(log_file, batch_size)\n",
        "\n",
        "    #     # Compile the model\n",
        "    #     optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    #     self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    #     # Train the model\n",
        "    #     self.model.fit(dataset, epochs=epochs)\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the model and tokenizer\"\"\"\n",
        "        os.makedirs(self.save_path, exist_ok=True)\n",
        "        self.model.save(f\"{self.save_path}/model.h5\")\n",
        "        self.tokenizer.save_pretrained(self.save_path)\n",
        "        print(f\"Model saved to {self.save_path}\")\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the model and tokenizer\"\"\"\n",
        "        self.model = tf.keras.models.load_model(f\"{self.save_path}/model.h5\", custom_objects={'LogAnomalyDetector': LogAnomalyDetector})\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.save_path)\n",
        "        print(\"Model loaded successfully\")\n",
        "\n",
        "    def detect_anomaly(self, log_message, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Predict anomaly score for a single log message.\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer(\n",
        "            log_message,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "\n",
        "        # Get prediction\n",
        "        anomaly_score = self.model([inputs['input_ids'], inputs['attention_mask']])\n",
        "        score = float(anomaly_score[0][0])\n",
        "\n",
        "        return {\n",
        "            'log_message': log_message,\n",
        "            'anomaly_score': score,\n",
        "            'is_anomaly': score > threshold\n",
        "        }\n"
      ],
      "metadata": {
        "id": "WfJWE0_5pPFx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from clearml import Task\n",
        "\n",
        "# Initialize ClearML task\n",
        "task = Task.init(\n",
        "    project_name=\"GenAI_OpenStack_RCA\",  # Project name\n",
        "    task_name=\"train_AnomalyModel\",     # Task name\n",
        "    task_type=Task.TaskTypes.training   # Task type\n",
        ")\n",
        "\n",
        "def main():\n",
        "    # Initialize pipeline\n",
        "    print(\"Initialize pipeline\")\n",
        "    pipeline = LogAnomalyDetectionPipeline()\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Train the model\")\n",
        "    pipeline.train_model(\n",
        "        log_file='/content/OpenStack_2k.log_structured.csv',\n",
        "        epochs=3\n",
        "    )\n",
        "\n",
        "    # # Save the model\n",
        "    # print(\"Save the model\")\n",
        "    # pipeline.save_model()\n",
        "\n",
        "    # # Load the model\n",
        "    # print(\"Load the model\")\n",
        "    # pipeline.load_model()\n",
        "\n",
        "    # Example anomaly detection\n",
        "    print(\"Example anomaly detection\")\n",
        "    sample_log = \"nova-compute.log.1.2017-05-16_13:55:31\t2017-05-16\t00:00:13.658\t2931\tINFO\tnova.compute.resource_tracker\treq-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -\tAuditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us\tE28\tAuditing locally available compute resources for node <*>\"\n",
        "    anomaly_result = pipeline.detect_anomaly(sample_log)\n",
        "    print(anomaly_result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "oreMT-flrCYG",
        "outputId": "4281bdc8-bc10-47ea-d5ef-901c7fdeeecc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize pipeline\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train the model\n",
            "Epoch 1/3\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 117ms/step - accuracy: 1.0000 - loss: 0.1782\n",
            "Epoch 2/3\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 125ms/step - accuracy: 1.0000 - loss: 0.0269\n",
            "Epoch 3/3\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 0.0105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example anomaly detection\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Exception encountered when calling LogAnomalyDetector.call().\n\n\u001b[1mlist indices must be integers or slices, not str\u001b[0m\n\nArguments received by LogAnomalyDetector.call():\n  • inputs=['tf.Tensor(shape=(1, 128), dtype=int32)', 'tf.Tensor(shape=(1, 128), dtype=int32)']\n  • training=False",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6d489b7fa849>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-6d489b7fa849>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Example anomaly detection\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0msample_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nova-compute.log.1.2017-05-16_13:55:31        2017-05-16      00:00:13.658    2931    INFO    nova.compute.resource_tracker   req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -      Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us  E28     Auditing locally available compute resources for node <*>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0manomaly_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomaly_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-136a7c6f237c>\u001b[0m in \u001b[0;36mdetect_anomaly\u001b[0;34m(self, log_message, threshold)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# Get prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0manomaly_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomaly_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-136a7c6f237c>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Access input_ids and attention_mask using keys instead of unpacking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mbert_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling LogAnomalyDetector.call().\n\n\u001b[1mlist indices must be integers or slices, not str\u001b[0m\n\nArguments received by LogAnomalyDetector.call():\n  • inputs=['tf.Tensor(shape=(1, 128), dtype=int32)', 'tf.Tensor(shape=(1, 128), dtype=int32)']\n  • training=False"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Previous experiment"
      ],
      "metadata": {
        "id": "3xCC0iw_nUz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "c311p245Eq_U",
        "outputId": "91a94fa7-dd99-481a-e37d-19bd5a2c6ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize pipeline\n",
            "Train the model\n",
            "LogAnomalyDetectionPipeline: Prepare dataloader\n",
            "LogAnomalyDetectionPipeline: Initialize trainer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogAnomalyDetectionPipeline: Train the model\n",
            "Epoch 1/3, Loss: 0.066974714551121\n",
            "Epoch 2/3, Loss: 0.009382738987915218\n",
            "Epoch 3/3, Loss: 0.004826889187097549\n",
            "Save the model\n",
            "Model saved to ./logbert_anomaly_model\n",
            "Load the model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1c53249b26eb>:181: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model = torch.load(f'{self.save_path}/model.pth') # Load the entire model\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1c53249b26eb>\u001b[0m in \u001b[0;36m<cell line: 248>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-1c53249b26eb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Load the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# Example anomaly detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-1c53249b26eb>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{self.save_path}/model.pth'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Load the entire model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from typing import List, Dict, Any\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    BertModel,\n",
        "    BertTokenizer,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class LogDataset(Dataset):\n",
        "    \"\"\"Custom dataset for log entries\"\"\"\n",
        "    def __init__(self, logs: List[str], tokenizer, max_length: int = 128):\n",
        "        self.logs = logs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.logs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        log = self.logs[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            log,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n",
        "class LogAnomalyDetector(nn.Module):\n",
        "    \"\"\"Unsupervised Anomaly Detection Model\"\"\"\n",
        "    def __init__(self, base_model: str = 'bert-base-uncased'):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(base_model)\n",
        "        self.anomaly_head = nn.Sequential(\n",
        "            nn.Linear(self.bert.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Extract BERT embeddings\n",
        "        bert_output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        # Use [CLS] token representation\n",
        "        cls_embedding = bert_output.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Anomaly scoring\n",
        "        anomaly_score = self.anomaly_head(cls_embedding)\n",
        "        return anomaly_score\n",
        "\n",
        "class LogAnomalyTrainer:\n",
        "    \"\"\"Trainer for Unsupervised Anomaly Detection\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        learning_rate: float = 2e-5,\n",
        "        device: str = None\n",
        "    ):\n",
        "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = model.to(self.device)\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "    def contrastive_loss(self, normal_embeddings, anomaly_embeddings):\n",
        "        \"\"\"Contrastive learning loss for anomaly detection\"\"\"\n",
        "        normal_distances = torch.cdist(normal_embeddings, normal_embeddings)\n",
        "        anomaly_distances = torch.cdist(anomaly_embeddings, anomaly_embeddings)\n",
        "\n",
        "        # Maximize inter-cluster distance, minimize intra-cluster distance\n",
        "        return torch.mean(normal_distances) - torch.mean(anomaly_distances)\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        dataloader: DataLoader,\n",
        "        epochs: int = 15,\n",
        "        verbose: bool = True\n",
        "    ):\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "\n",
        "            for batch in dataloader:\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "\n",
        "                # Compute anomaly scores\n",
        "                anomaly_scores = self.model(input_ids, attention_mask)\n",
        "\n",
        "                # Compute unsupervised loss\n",
        "                loss = torch.mean(anomaly_scores)  # Encourage low anomaly scores\n",
        "\n",
        "                # Backpropagation\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader)}\")\n",
        "\n",
        "class LogAnomalyDetectionPipeline:\n",
        "    \"\"\"End-to-end pipeline for log anomaly detection\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = 'bert-base-uncased',\n",
        "        save_path: str = './logbert_anomaly_model'\n",
        "    ):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = LogAnomalyDetector(base_model=model_name)\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def prepare_data(\n",
        "        self,\n",
        "        log_file: str,\n",
        "        batch_size: int = 4\n",
        "    ) -> DataLoader:\n",
        "        # Load log data\n",
        "        log_data = pd.read_csv(log_file)\n",
        "        logs = log_data['EventTemplate'].tolist()\n",
        "\n",
        "        # Create dataset and dataloader\n",
        "        dataset = LogDataset(logs, self.tokenizer)\n",
        "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    def train_model(\n",
        "        self,\n",
        "        log_file: str,\n",
        "        epochs: int = 15,\n",
        "        learning_rate: float = 2e-6\n",
        "    ):\n",
        "        # Prepare dataloader\n",
        "        print(\"LogAnomalyDetectionPipeline: Prepare dataloader\")\n",
        "        dataloader = self.prepare_data(log_file)\n",
        "\n",
        "        # Initialize trainer\n",
        "        print(\"LogAnomalyDetectionPipeline: Initialize trainer\")\n",
        "        trainer = LogAnomalyTrainer(\n",
        "            model=self.model,\n",
        "            learning_rate=learning_rate\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"LogAnomalyDetectionPipeline: Train the model\")\n",
        "        trainer.train(dataloader, epochs)\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the fine-tuned model\"\"\"\n",
        "        os.makedirs(self.save_path, exist_ok=True)\n",
        "        torch.save(self.model.state_dict(), f'{self.save_path}/model.pth')\n",
        "        # torch.save(self.model, f'{self.save_path}/model2.pth')\n",
        "        # torch.save(self.model, f'{self.save_path}/model2.pkl')\n",
        "        # self.model.save(f\"{self.save_path}/model.h5\")\n",
        "        self.tokenizer.save_pretrained(self.save_path)\n",
        "        print(f\"Model saved to {self.save_path}\")\n",
        "\n",
        "    # def load_model(self):\n",
        "    #     \"\"\"Load the saved model\"\"\"\n",
        "    #     self.model.load_state_dict(torch.load(f'{self.save_path}/model.pth'))\n",
        "    #     self.tokenizer = BertTokenizer.from_pretrained(self.save_path)\n",
        "    #     self.model.eval()\n",
        "    #     print(\"Model loaded successfully\")\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the saved model\"\"\"\n",
        "        # self.model.load_state_dict(torch.load(f'{self.save_path}/model.pth')) # Original line causing the error\n",
        "        self.model = torch.load(f'{self.save_path}/model.pth') # Load the entire model\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.save_path)\n",
        "        self.model.eval()\n",
        "        print(\"Model loaded successfully\")\n",
        "\n",
        "    def detect_anomaly(self, log_message: str, threshold: float = 0.5) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Detect anomaly for a single log message\n",
        "\n",
        "        Args:\n",
        "            log_message (str): Log entry to check\n",
        "            threshold (float): Anomaly score threshold\n",
        "\n",
        "        Returns:\n",
        "            Dict with anomaly detection results\n",
        "        \"\"\"\n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer(\n",
        "            log_message,\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(self.model.bert.embeddings.word_embeddings.weight.device) # Move inputs to the same device as the model's word embeddings\n",
        "\n",
        "        # Compute anomaly score\n",
        "        with torch.no_grad():\n",
        "            anomaly_score = self.model(\n",
        "                inputs['input_ids'],\n",
        "                inputs['attention_mask']\n",
        "            )\n",
        "\n",
        "        # Convert to numpy for easier processing\n",
        "        score = anomaly_score.cpu().numpy()[0][0]\n",
        "\n",
        "        return {\n",
        "            'log_message': log_message,\n",
        "            'anomaly_score': float(score),\n",
        "            'is_anomaly': bool(score > threshold)\n",
        "        }\n",
        "\n",
        "# Example Usage\n",
        "def main():\n",
        "    # Initialize pipeline\n",
        "    print(\"Initialize pipeline\")\n",
        "    pipeline = LogAnomalyDetectionPipeline()\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Train the model\")\n",
        "    pipeline.train_model(\n",
        "        log_file='/content/OpenStack_2k.log_structured.csv',\n",
        "        epochs=3\n",
        "    )\n",
        "\n",
        "    # Save the model\n",
        "    print(\"Save the model\")\n",
        "    pipeline.save_model()\n",
        "\n",
        "    # Load the model\n",
        "    print(\"Load the model\")\n",
        "    pipeline.load_model()\n",
        "\n",
        "    # Example anomaly detection\n",
        "    print(\"Example anomaly detection\")\n",
        "    sample_log = \"nova-compute.log.1.2017-05-16_13:55:31\t2017-05-16\t00:00:13.658\t2931\tINFO\tnova.compute.resource_tracker\treq-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -\tAuditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us\tE28\tAuditing locally available compute resources for node <*>\"\n",
        "    anomaly_result = pipeline.detect_anomaly(sample_log)\n",
        "    print(anomaly_result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8FS39-97nO7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5YsLZ-sUnS5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4ay5Q1AgnNeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XmfDL8p0nLbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vK0y3ZxknMIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import List\n",
        "import re\n",
        "\n",
        "class LogGenerator:\n",
        "    model_name: str = 'bert-base-uncased',\n",
        "    save_path: str = './logbert_anomaly_model'\n",
        "    @staticmethod\n",
        "    def generate_standard_logs(num_logs: int = 20) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate standard OpenStack Nova Compute log messages\n",
        "        \"\"\"\n",
        "        standard_templates = [\n",
        "            # Standard audit logs\n",
        "            \"nova-compute.log\t{date}\t{time}\t{pid}\tINFO\tnova.compute.resource_tracker\t{req_id}\tAuditing locally available compute resources for node {node_name}\",\n",
        "\n",
        "            # Instance lifecycle logs\n",
        "            \"nova-compute.log\t{date}\t{time}\t{pid}\tINFO\tnova.compute.manager\t{req_id}\tCreating instance {instance_id} on hypervisor\",\n",
        "\n",
        "            # Resource allocation logs\n",
        "            \"nova-compute.log\t{date}\t{time}\t{pid}\tINFO\tnova.compute.resource_tracker\t{req_id}\tAllocated {resource_type} for instance {instance_id}\",\n",
        "\n",
        "            # Network configuration logs\n",
        "            \"nova-compute.log\t{date}\t{time}\t{pid}\tINFO\tnova.network.manager\t{req_id}\tConfiguring network for instance {instance_id}\",\n",
        "        ]\n",
        "\n",
        "        logs = []\n",
        "        for _ in range(num_logs):\n",
        "            log = random.choice(standard_templates).format(\n",
        "                date=f\"2024-{random.randint(1, 12):02d}-{random.randint(1, 28):02d}\",\n",
        "                time=f\"{random.randint(0, 23):02d}:{random.randint(0, 59):02d}:{random.randint(0, 59):02d}.{random.randint(0, 999):03d}\",\n",
        "                pid=random.randint(1000, 9999),\n",
        "                req_id=f\"req-{random.randint(10000, 99999)}\",\n",
        "                node_name=f\"cp-{random.randint(1, 10)}.hypervisor.cluster\",\n",
        "                instance_id=f\"instance-{random.randint(1000, 9999)}\",\n",
        "                resource_type=random.choice(['CPU', 'Memory', 'Storage', 'Network'])\n",
        "            )\n",
        "            logs.append(log)\n",
        "\n",
        "        return logs\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_anomalous_logs(num_logs: int = 5) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate anomalous OpenStack Nova Compute log messages\n",
        "        \"\"\"\n",
        "        anomalous_templates = [\n",
        "            # Unusual error messages\n",
        "            \"nova-compute.log\t{date}\t{time}\t{pid}\tERROR\tnova.compute.manager\t{req_id}\tUnexpected error during instance creation: {error_details}\",\n",
        "\n",
        "            # Resource exhaustion\n",
        "            \"nova-compute.log\t{date}\t{time}\t{pid}\tCRITICAL\tnova.compute.resource_tracker\t{req_id}\tCritical: No available resources for new instance allocation\",\n",
        "\n",
        "            # Unusual process terminations\n",
        "            \"nova-compute.log\t{date}\t{time}\t{pid}\tWARNING\tnova.compute.manager\t{req_id}\tCompute service unexpectedly terminated\",\n",
        "\n",
        "            # Potential security-related anomalies\n",
        "            \"nova-compute.log\t{date}\t{time}\t{pid}\tALERT\tnova.security.monitor\t{req_id}\tPotential unauthorized access attempt detected\",\n",
        "        ]\n",
        "\n",
        "        logs = []\n",
        "        for _ in range(num_logs):\n",
        "            log = random.choice(anomalous_templates).format(\n",
        "                date=f\"2024-{random.randint(1, 12):02d}-{random.randint(1, 28):02d}\",\n",
        "                time=f\"{random.randint(0, 23):02d}:{random.randint(0, 59):02d}:{random.randint(0, 59):02d}.{random.randint(0, 999):03d}\",\n",
        "                pid=random.randint(1000, 9999),\n",
        "                req_id=f\"req-{random.randint(10000, 99999)}\",\n",
        "                error_details=random.choice([\n",
        "                    \"Memory allocation failure\",\n",
        "                    \"Disk I/O error\",\n",
        "                    \"Network configuration mismatch\",\n",
        "                    \"Unexpected kernel panic\"\n",
        "                ])\n",
        "            )\n",
        "            logs.append(log)\n",
        "\n",
        "        return logs\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_log_dataset(self, anomaly_detector):\n",
        "        \"\"\"\n",
        "        Prepare a dataset of logs for testing\n",
        "        \"\"\"\n",
        "        # Generate standard and anomalous logs\n",
        "        standard_logs = LogGenerator.generate_standard_logs(50)\n",
        "        anomalous_logs = LogGenerator.generate_anomalous_logs(20)\n",
        "\n",
        "        # Combine logs\n",
        "        all_logs = standard_logs + anomalous_logs\n",
        "        random.shuffle(all_logs)\n",
        "\n",
        "        # Detect anomalies\n",
        "        anomaly_results = []\n",
        "        for log in all_logs:\n",
        "            result = anomaly_detector.detect_anomaly(log)\n",
        "            anomaly_results.append(result)\n",
        "\n",
        "        return anomaly_results\n",
        "\n",
        "    @staticmethod\n",
        "    def analyze_anomaly_detection(self, anomaly_results):\n",
        "        \"\"\"\n",
        "        Analyze the anomaly detection results\n",
        "        \"\"\"\n",
        "        total_logs = len(anomaly_results)\n",
        "        detected_anomalies = [\n",
        "            result for result in anomaly_results\n",
        "            if result['is_anomaly']\n",
        "        ]\n",
        "\n",
        "        print(\"\\n--- Anomaly Detection Analysis ---\")\n",
        "        print(f\"Total Logs Analyzed: {total_logs}\")\n",
        "        print(f\"Detected Anomalies: {len(detected_anomalies)}\")\n",
        "        print(f\"Anomaly Detection Rate: {len(detected_anomalies)/total_logs*100:.2f}%\")\n",
        "\n",
        "        # Print top 5 most anomalous logs\n",
        "        print(\"\\nTop 5 Most Anomalous Logs:\")\n",
        "        top_anomalies = sorted(\n",
        "            anomaly_results,\n",
        "            key=lambda x: x['anomaly_score'],\n",
        "            reverse=True\n",
        "        )[:5]\n",
        "\n",
        "        for anomaly in top_anomalies:\n",
        "            print(f\"Log: {anomaly['log_message']}\")\n",
        "            print(f\"Anomaly Score: {anomaly['anomaly_score']}\")\n",
        "            print(f\"Is Anomaly: {anomaly['is_anomaly']}\\n\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model():\n",
        "        \"\"\"Load the saved model\"\"\"\n",
        "        save_path: str = './logbert_anomaly_model'\n",
        "        model.load_state_dict(torch.load(f'{save_path}/model.pth'))\n",
        "        tokenizer = BertTokenizer.from_pretrained(save_path)\n",
        "        model.eval()\n",
        "        print(\"Model loaded successfully\")\n",
        "\n",
        "# Main Execution\n",
        "def main():\n",
        "    # Initialize Anomaly Detector (from previous implementation)\n",
        "    anomaly_detector = LogGenerator()\n",
        "\n",
        "    # Load pre-trained model (optional)\n",
        "    # anomaly_detector.load_model()\n",
        "\n",
        "    # # Train the model (uncomment if not pre-trained)\n",
        "    # log_file = '/content/OpenStack_2k.log_structured.csv'\n",
        "    # anomaly_detector.train(log_file, epochs=3)\n",
        "\n",
        "    # # Save the model\n",
        "    # anomaly_detector.save_model()\n",
        "    anomaly_detector.load_model()\n",
        "\n",
        "    # Specific log prediction\n",
        "    sample_log = \"nova-compute.log.1.2017-05-16_13:55:31\t2017-05-16\t00:00:13.658\t2931\tINFO\tnova.compute.resource_tracker\treq-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -\tAuditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us\tE28\tAuditing locally available compute resources for node <*>\"\n",
        "\n",
        "    # Detect anomaly for specific log\n",
        "    result = anomaly_detector.detect_anomaly(sample_log)\n",
        "    print(\"\\nSpecific Log Anomaly Detection:\")\n",
        "    print(result)\n",
        "\n",
        "    # Comprehensive log analysis\n",
        "    anomaly_results = prepare_log_dataset(self,anomaly_detector)\n",
        "    analyze_anomaly_detection(anomaly_results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "UMuVOFjAbOVZ",
        "outputId": "211c943b-81f0-4b46-d02f-7c7834862a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'self' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4bc5642680db>\u001b[0m in \u001b[0;36m<cell line: 167>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-4bc5642680db>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;31m# # Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;31m# anomaly_detector.save_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0manomaly_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# Specific log prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-4bc5642680db>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;34m\"\"\"Load the saved model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{self.save_path}/model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmDcJjCx-GhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "csv_file = \"/content/OpenStack_2k.log_structured.csv\"\n",
        "log_data = pd.read_csv(csv_file)\n",
        "# log_data"
      ],
      "metadata": {
        "id": "vEhXfCjZFBuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New code start"
      ],
      "metadata": {
        "id": "vhsmBKVrr5iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the BERT tokenizer and model\n",
        "model_name = \"bert-base-uncased\"  # Choose the BERT model you prefer\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Load the log data\n",
        "log_data = pd.read_csv(\"/content/OpenStack_2k.log_structured.csv\")\n",
        "\n",
        "# Step 1: Batch Tokenization and Preprocessing\n",
        "# Optimization: Use batch tokenization to speed up processing\n",
        "logs = log_data[\"EventTemplate\"].tolist()\n",
        "tokenized = tokenizer(\n",
        "    logs,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=100,  # Set desired maximum length\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Extract input IDs and attention masks\n",
        "log_tensors = tokenized[\"input_ids\"]\n",
        "attention_masks = tokenized[\"attention_mask\"]\n",
        "\n",
        "# Step 2: Compute Log Embeddings\n",
        "log_embeddings = []\n",
        "batch_size = 32  # Process logs in batches to optimize GPU memory usage\n",
        "\n",
        "for i in range(0, len(log_tensors), batch_size):\n",
        "    input_ids_batch = log_tensors[i : i + batch_size]\n",
        "    attn_mask_batch = attention_masks[i : i + batch_size]\n",
        "\n",
        "    # Get embeddings for the batch\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids_batch, attention_mask=attn_mask_batch)\n",
        "    # Extract the embedding for [CLS] token\n",
        "    batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "    log_embeddings.extend(batch_embeddings)# Step 3: Compute Anomaly Detection Using Cosine Similarity\n",
        "# Check if log_embeddings has valid data\n",
        "if len(log_embeddings) > 0:\n",
        "    # Calculate the centroid (mean embedding) for all log embeddings\n",
        "    mean_embedding = np.mean(log_embeddings, axis=0)\n",
        "else:\n",
        "    mean_embedding = None\n",
        "\n",
        "# Define anomaly threshold (can be tuned)\n",
        "anomaly_threshold = 0.95  # Adjust as needed for anomaly sensitivity\n",
        "\n",
        "# Compute cosine similarity for each log embedding\n",
        "anomalous_indices = []\n",
        "if mean_embedding is not None:\n",
        "    for i, log_embedding in enumerate(log_embeddings):\n",
        "        similarity_score = cosine_similarity([log_embedding], [mean_embedding])[0][0]\n",
        "        # Mark logs as anomalies if similarity score falls below the threshold\n",
        "        if similarity_score < anomaly_threshold:\n",
        "            anomalous_indices.append(i)\n",
        "            print(f\"Anomaly detected: Log {i} (Similarity: {similarity_score:.4f})\")\n",
        "\n",
        "# Add 'Anomaly' column to log_data\n",
        "log_data[\"Anomaly\"] = 0  # Initialize with all logs as normal\n",
        "log_data.loc[anomalous_indices, \"Anomaly\"] = 1  # Mark anomalies in the DataFrame\n",
        "\n",
        "# Save labeled data to a new file\n",
        "output_file = \"OpenStack_2k.log_labeled.csv\"\n",
        "log_data.to_csv(output_file, index=False)\n",
        "print(f\"Saved labeled data to {output_file}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "js2Jg21Er5VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## new code end"
      ],
      "metadata": {
        "id": "aLvPmTmar9xp"
      }
    }
  ]
}