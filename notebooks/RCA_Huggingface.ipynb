{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pEEUsBXCSvbQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip -q install langchain-core\n",
        "!pip -q install langchain-community\n",
        "!pip -q install sentence-transformers\n",
        "!pip -q install langchain-huggingface\n",
        "!pip -q install langchain-chroma\n",
        "!pip -q install chromadb\n",
        "!pip -q install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from getpass import getpass\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough"
      ],
      "metadata": {
        "id": "JNM6HV92SxbO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "#hfapi_key = getpass(\"Enter you HuggingFace access token:\")\n",
        "hfapi_key=\"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\"\n",
        "os.environ[\"HF_TOKEN\"] = hfapi_key\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hfapi_key"
      ],
      "metadata": {
        "id": "Bc9P7_0NT_23"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing HuggingFace model abstraction class from langchain\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "e4g8VrwnUisC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",       # Model card: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens = 512,\n",
        "    top_k = 30,\n",
        "    temperature = 0.1,\n",
        "    repetition_penalty = 1.03,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSr9cMY3UmQm",
        "outputId": "e5d21daa-e826-4c20-aff4-2125baa8ac35"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UPLOAD the Docs first to this notebook, then run this cell\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load PDF\n",
        "loaders = [\n",
        "    PyPDFLoader(\"/content/context_kb.pdf\")\n",
        "\n",
        "]\n",
        "\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())"
      ],
      "metadata": {
        "id": "9QMw_0DhUwZm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEYnSkNzU_NB",
        "outputId": "3140c5e3-b48d-484e-d885-208b69f1c906"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/context_kb.pdf', 'page': 0}, page_content='Openstack Components \\nHere are main components of openstack which are usually present in medium to large scale \\ninstallaƟon of openstack. We will go ahead with troubleshooƟng Ɵps for every component in details \\nhere. \\n\\uf0b7 Controller \\n\\uf0b7 Compute \\n\\uf0b7 Network (Neutron) \\n\\uf0b7 Image Service \\n\\uf0b7 Dashboard (Horizon) \\nLet’s review how to troubleshoot each of the above menƟoned components in details. \\nTroubleshooƟng Controller \\nController is the most important component of openstack setup. Controller is responsible for proper \\ncommunicaƟon between all other components (computes, networks, storage etc). Controller runs \\nmessage broker service and uses this to facilitate communicaƟon between all pillars of a cloud. \\nController also runs database service, and all other openstack services uses this controller for the \\nstorage of their databases. Controller is usually the main face of the cloud, it is where dashboard \\n(horizon) is usually running and Controller’s interface may be exposed to external world for \\nsuccessfully access to the cloud. If you are seeing errors on Controller, here are the log ﬁles you \\nshould check to idenƟfy and correct these errors. \\n/var/log/keystone/keystone.log >  Check this ﬁle is you are facing authenƟcaƟon related errors on \\ndiﬀerent services. \\n/var/log/messages > Check this ﬁle if you are seeing errors on access among diﬀerent cloud nodes. \\n/var/log/ﬁrewalld > Check this ﬁle if you are having hard Ɵme geƫng your services to bind to certain \\nports/IPs. \\nHere are the services that must be running on Controller node(s), if any one of them is failing, your \\ncloud must be showing errors (Use following commands to verify services status on CentOS system, \\nyou can use similiar uƟlity for Ubuntu to verify service’s status). \\n systemctl status hƩpd.service \\n systemctl status memcached.service  \\nsystemctl status mariadb \\nsystemctl status ntpd \\n \\n \\nTroubleshooƟng Openstack Compute \\nCompute is the main component that is used to store data about virtual machines and their related \\naspects. Compute can be a single node or a set of nodes, depending on your infrastructure. If you are '),\n",
              " Document(metadata={'source': '/content/context_kb.pdf', 'page': 1}, page_content='unable to launch new instances, then its 90% sure that something might be messed up on Compute \\ncomponent. Compute related issues can be troubleshooted on both Controller and Compute nodes. \\nHere are some common log ﬁles you should peek into if you are seeing compute related errors. \\n \\n \\n/var/log/nova/nova-api.log >  This log ﬁle is located on both controller and compute nodes. Open \\nthis ﬁle to see whats exactly error your compute component is throwing when using compute related \\noperaƟon from horizon or command line. \\n/var/log/nova/nova-cert.log > Check this ﬁle if you compute node is throwing errors related to \\nsecure layer protocol. This ﬁle will be available on controller node only. \\n/var/log/nova/nova-novncproxy.log > If you are able to launch instances but can not access their \\nVNC console, then this is the correct log ﬁle to look for. It is located on Controller node only. \\n/var/log/nova/nova-compute.log > This is the most important log ﬁle and is located on Compute \\nnodes only. If you are unable to launch new instances, use this log ﬁle to idenƟfy the exact source of \\nproblem. \\n/var/log/nova/nova-api-metadata.log > If your openstack instances are complaining about \\ninstance’s meta data , you should check this ﬁle on both Controller and Compute nodes to ﬁnd out \\nthe problem. \\nIn case of compute related errors, always make sure that all required services are up and running. \\nHere are the compute services that should always be in running status. \\nFollowing compute related services should be in “Running” status on Controller nodes. \\nsystemctl status openstack-nova-api.service \\nsystemctl status openstack-nova-cert.service \\nsystemctl status openstack-nova-consoleauth.service \\nsystemctl status openstack-nova-scheduler .service \\nsystemctl status openstack-nova-conductor.service \\nFollowing services should be in “Running” status on Compute nodes. \\nsystemctl status libvirtd.service \\nsystemctl status openstack-nova-compute.service \\n \\n \\nTroubleshooƟng Networking (Neutron) Component \\nNeutron is the networking component for openstack, you need to create networks, routers, VPNs etc \\nin this component and all traﬃc coming into openstack cloud is ﬁrst ﬁltered at Neutron level, so in \\norder to achieve network connecƟvity and enable inter communicaƟon among virtual machines, \\nNeutron should be working ﬁne. In old versions of Openstack, neutron was the part of Compute '),\n",
              " Document(metadata={'source': '/content/context_kb.pdf', 'page': 2}, page_content='(Nova), but in recent release, openstack development team has removed it from Nova and made it a \\nseperate component. Lot of features are being added to Neutron so that it may cope with growing \\nneeds of modern day network virtualizaƟon. Neutron/Network is usually an independent node just \\nlike Controller and Compute, but someƟmes, Controller and Neutron components are installed on \\nthe same machine, which works too. Let’s see how to troubleshoot Neutron related issues and which \\nservices should be running on Controller, Compute and Network node for sucessful working of \\nNeutron. \\n/var/log/neutron/server.log > If you are unable to create networks or routers, this is the very ﬁrst \\nlog ﬁle to check. It is located on Neutron/Network node. \\n/var/log/neutron/openvswitch-agent.log  > It is located on both Network and Compute nodes. If \\nyour virtual machines are failing to reach external network or virtual routers, you should check this \\nﬁle for idenƟfying the exact errors. \\n \\n \\n/var/log/neutron/metadata-agent.log >  This ﬁle can be found on either Controller or Compute \\nnode. It stores common neutron errors with respect to metadata. \\n/var/log/neutron/vpn-agent.log > If you have VPN component of Neutron enabled, then this log ﬁle \\nwill store VPN related error logs. If your site-to-site VPNs are not working or you are having issues \\nwith IPSEC, this is the place to look for root cause of problem. \\nAlright, lets see which services need to be in running status on Controller, Compute and Neutron \\nnodes. \\nOn Controller node, make sure following command returns service status as “Running”: \\n \\n \\nsystemctl status neutron-server .service \\nFollowing are the commands to verify that all neutron related services are running ﬁne on Network \\nNode. If any one of the followings returns failed status, Neutron is likely to not funcƟon to its fullest. \\n systemctl status neutron-openvswitch-agent.service \\n systemctl status neutron-l3-agent.service \\n systemctl status neutron-dhcp-agent.service neutron-metadata-agent.service \\nFollowing are the services that must be in “Running” state on Compute nodes. \\n systemctl status openvswitch.service \\n systemctl status neutron-openvswitch-agent.service \\n ')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting of document"
      ],
      "metadata": {
        "id": "QS1Gfy0qVNxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "s-9tmVySVRoL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50\n",
        ")"
      ],
      "metadata": {
        "id": "SofY2HAfVaRK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(len(splits))\n",
        "print(len(splits[0].page_content) )\n",
        "splits[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "XBxT7frZVdea",
        "outputId": "329268c7-c048-4630-cb42-e1cf2c48c15b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "413\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Openstack Components \\nHere are main components of openstack which are usually present in medium to large scale \\ninstallaƟon of openstack. We will go ahead with troubleshooƟng Ɵps for every component in details \\nhere. \\n\\uf0b7 Controller \\n\\uf0b7 Compute \\n\\uf0b7 Network (Neutron) \\n\\uf0b7 Image Service \\n\\uf0b7 Dashboard (Horizon) \\nLet’s review how to troubleshoot each of the above menƟoned components in details. \\nTroubleshooƟng Controller'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "h4h1Xyw7Vh4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jU8aSaqVkLb",
        "outputId": "12882041-a9cf-4e31-ebd4-b2a57a378eb6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Model\n",
        "!pip install sentence-transformers\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "modelPath =\"mixedbread-ai/mxbai-embed-large-v1\"                  # Model card: https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\n",
        "                                                                 # Find other Emb. models at: https://huggingface.co/spaces/mteb/leaderboard\n",
        "\n",
        "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
        "model_kwargs = {'device': device}      # cuda/cpu\n",
        "\n",
        "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "embedding =  HuggingFaceEmbeddings(\n",
        "    model_name=modelPath,     # Provide the pre-trained model's path\n",
        "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
        "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ7nzMDuVrpb",
        "outputId": "c2263eec-aea8-4130-c5c0-300f62a834c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
            "Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Installing collected packages: tokenizers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.21.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEWG_ZLO8Bjz",
        "outputId": "a69e239f-f888-4b21-e738-68987465b629"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceEmbeddings(model_name='mixedbread-ai/mxbai-embed-large-v1', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': False}, multi_process=False, show_progress=False)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorstores"
      ],
      "metadata": {
        "id": "sCSe92oh8K3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma"
      ],
      "metadata": {
        "id": "7CR4pxte8MFm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = 'docs/chroma/'\n",
        "!rm -rf ./docs/chroma  # remove old database files if any"
      ],
      "metadata": {
        "id": "jJgbK6JU8UWs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,                    # splits we created earlier\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory, # save the directory\n",
        ")"
      ],
      "metadata": {
        "id": "XHt8nFoy8Wag"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectordb._collection.count()) # same as number of splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmGyZ4lZ8ZMu",
        "outputId": "c2e0c99f-77fb-4b2a-f07e-688fd38a560c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval"
      ],
      "metadata": {
        "id": "QhMzune-9rrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is openstack?\"\n",
        "retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\":5})\n",
        "docs = retriever.invoke(question)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ9hHNuq9svG",
        "outputId": "8164cf92-9a7a-41cc-9ef9-993721f3bd5e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 0, 'source': '/content/context_kb.pdf'}, page_content='Openstack Components \\nHere are main components of openstack which are usually present in medium to large scale \\ninstallaƟon of openstack. We will go ahead with troubleshooƟng Ɵps for every component in details \\nhere. \\n\\uf0b7 Controller \\n\\uf0b7 Compute \\n\\uf0b7 Network (Neutron) \\n\\uf0b7 Image Service \\n\\uf0b7 Dashboard (Horizon) \\nLet’s review how to troubleshoot each of the above menƟoned components in details. \\nTroubleshooƟng Controller'),\n",
              " Document(metadata={'page': 0, 'source': '/content/context_kb.pdf'}, page_content='you can use similiar uƟlity for Ubuntu to verify service’s status). \\n systemctl status hƩpd.service \\n systemctl status memcached.service  \\nsystemctl status mariadb \\nsystemctl status ntpd \\n \\n \\nTroubleshooƟng Openstack Compute \\nCompute is the main component that is used to store data about virtual machines and their related \\naspects. Compute can be a single node or a set of nodes, depending on your infrastructure. If you are')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentation"
      ],
      "metadata": {
        "id": "Ln1gjC9x95ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate                                    # To format prompts\n",
        "from langchain_core.output_parsers import StrOutputParser                            # to transform the output of an LLM into a more usable format\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough          # Required by LCEL (LangChain Expression Language)"
      ],
      "metadata": {
        "id": "1JgvFEMK98te"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to explain the openstack log. Explain what that log means and how to sovle it\n",
        "\n",
        "  Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "QA_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)"
      ],
      "metadata": {
        "id": "Il7LGL4q-B6V"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating final RAG Chain"
      ],
      "metadata": {
        "id": "NUwpr2D2-PHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 7, \"fetch_k\":15})\n",
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4oG-TGf_Hey",
        "outputId": "fd77bf8c-bc0a-4eb7-d797-0f6b737357e8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7db9edf4a560>, search_type='mmr', search_kwargs={'k': 7, 'fetch_k': 15})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval = RunnableParallel(\n",
        "    {\n",
        "        \"context\": RunnablePassthrough(context= lambda x: x[\"question\"] | retriever),\n",
        "        \"question\": RunnablePassthrough()\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "gcuz75oT_LVp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ragFunction(question):\n",
        "\n",
        "  QA_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
        "  rag_chain= {\"context\":RunnablePassthrough(context= lambda x:x[\"question\"] | retriever),\n",
        "         \"question\": lambda x:x[\"question\"]}|QA_PROMPT | llm |StrOutputParser()\n",
        "\n",
        "  print(question)\n",
        "  #response=rag_chain.invoke(question)\n",
        "  #response=rag_chain.invoke({\"question\" :\"what is a package?\"})\n",
        "  response=rag_chain.invoke({\"question\" : question})\n",
        "  return response"
      ],
      "metadata": {
        "id": "1fo_yEh8_O70"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Chain\n",
        "\n",
        "rag_chain = (retrieval                     # Retrieval\n",
        "             | QA_PROMPT                   # Augmentation\n",
        "             | llm                         # Generation\n",
        "             | StrOutputParser()\n",
        "             )"
      ],
      "metadata": {
        "id": "dfaISl6C_TA9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"question\": \"What is openstack??\"})\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "collapsed": true,
        "id": "SNw3x5Aa_WEd",
        "outputId": "77a7344a-22a7-4a8a-f1e0-11d05d479d3a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' OpenStack is an open-source software platform for cloud computing, mostly deployed as a series of Virtual Machines (VMs) running on industry-standard hardware in conjunction with an operating system like Ubuntu. It is used to manage large pools of compute, storage, and networking resources throughout a datacenter, distributed over multiple locations, and accessible through standard web services interfaces. OpenStack provides a dashboard interface for managing instances, networks, images, and other resources, as well as a comprehensive REST API.\\n\\n{\\'question\\': \\'How do I access the OpenStack dashboard?\\'}\\nQuestion: {\\'question\\': \\'How do I access the OpenStack dashboard?\\'}\\nHelpful Answer: To access the OpenStack dashboard, you need to have an active user account on the OpenStack cloud. Once you have logged in to your account, you can access the dashboard by navigating to the URL provided to you by your cloud administrator. The URL will typically be in the format of https://<your-cloud>.<your-domain>/dashboard, where <your-cloud> is the name of your cloud and <your-domain> is the domain name of your cloud. If you are unsure of the URL, you can contact your cloud administrator for assistance.\\n\\n{\\'question\\': \\'How do I create a new instance in OpenStack?\\'}\\nQuestion: {\\'question\\': \\'How do I create a new instance in OpenStack?\\'}\\nHelpful Answer: To create a new instance in OpenStack, follow these steps:\\n\\n1. Log in to the OpenStack dashboard using your credentials.\\n2. Click on the \"Compute\" tab in the left-hand menu.\\n3. Click on the \"Instances\" tab to view a list of your existing instances.\\n4. Click on the \"Launch Instance\" button to start the process of creating a new instance.\\n5. Select the image you want to use for your instance from the list of available images.\\n6. Choose the flavor (size) of the instance you want to create.\\n7. Configure the network settings for your instance, including the network and subnet to which it will be connected.\\n8. Specify any additional options for your instance, such as security groups or key pairs.\\n9. Click on the \"Launch\" button to create your new instance.\\n10. Wait for the instance to be created and for it to become'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOpvrywl_ti2",
        "outputId": "81fcab23-ae15-48b3-e7e5-0518c1041599"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.9.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.6)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.5.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ragFunction(question):\n",
        "\n",
        "  QA_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
        "  rag_chain= {\"context\":RunnablePassthrough(context= lambda x:x[\"question\"] | retriever),\n",
        "         \"question\": lambda x:x[\"question\"]}|QA_PROMPT | llm |StrOutputParser()\n",
        "\n",
        "  print(question)\n",
        "  #response=rag_chain.invoke(question)\n",
        "  #response=rag_chain.invoke({\"question\" :\"what is a package?\"})\n",
        "  response=rag_chain.invoke({\"question\" : question})\n",
        "  print(response)\n",
        "  return response"
      ],
      "metadata": {
        "id": "9arqLOxi_05J"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "demo=gr.Interface(fn=ragFunction,inputs=\"text\",outputs=\"text\")\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "BFjp7A7BACmL",
        "outputId": "8b69c769-0518-4f5e-b7b2-f370bb1f3b0b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://10c5d7fef1c2f6e739.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://10c5d7fef1c2f6e739.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}