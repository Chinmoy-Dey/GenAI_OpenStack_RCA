# -*- coding: utf-8 -*-
"""RAG_OPENAI_ver2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BLFLU230BDz1R3CBqssN3wBaLsmxoYaa
"""


import openai
import os

f = open('../data/ts_openapi_key.txt')
api_key = f.read()
os.environ['OPENAI_API_KEY'] = api_key
openai.api_key= os.getenv('OPENAI_API_KEY')

from langchain_openai import ChatOpenAI

from langchain_openai import ChatOpenAI

from langchain_community.document_loaders import PyPDFLoader
# Load PDF
loaders = [
    # Duplicate documents on purpose
    PyPDFLoader("../data/context_kb.pdf"),

]
docs = []

for loader in loaders:
    docs.extend(loader.load())


#print(docs[0].page_content)

for loader in loaders:
    docs.extend(loader.load())

print(docs[0].page_content)

from langchain_text_splitters import RecursiveCharacterTextSplitter

# Split
#from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 500,
    chunk_overlap = 50
)

splits = text_splitter.split_documents(docs)
print(len(splits))
print(len(splits[0].page_content) )
splits[0].page_content

splits

from langchain.embeddings.openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings(model='text-embedding-ada-002')


from langchain_community.vectorstores import Chroma # Light-weight and in memory

persist_directory = 'docs/chroma_6/'

###
vectordb = Chroma.from_documents(
    documents=splits, # splits we created earlier
    embedding=embedding,
    persist_directory=persist_directory, # save the directory
)
###


#print(vectordb._collection.count()) # same as number of splites

retriever = vectordb.as_retriever(search_type="mmr",search_kwargs={"k": 7, "fetch_k":15})

llm_name = "gpt-4o-mini"
print(llm_name)

from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model_name=llm_name, temperature=0)

from langchain_core.prompts import PromptTemplate     #
from langchain_core.output_parsers import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough

# Build prompt
template = """Use the following pieces of context to explain the openstack log. Explain what that log means and a potential solution. There are more than one log. Identify what these logs mean

  {context}
  Question: {question}
  Helpful Answer:"""


def ragFunction_openai(question):

  QA_PROMPT = PromptTemplate(input_variables=["context", "question"],template=template,)
  rag_chain= {"context":RunnablePassthrough(context= lambda x:x["question"] | retriever),
         "question": lambda x:x["question"]}|QA_PROMPT | llm |StrOutputParser()

  print(question)
  #response=rag_chain.invoke(question)
  #response=rag_chain.invoke({"question" :"what is a package?"})
  response=rag_chain.invoke({"question" : question})
  print (response)
  return response

#log="<*> POST <*>status: <*> len: <*> time: <*>.<*>"

#ragFunction(log)